---
description: "TAGS: [task-generation,automation,scripts,workflows,execution] | TRIGGERS: task generation,protocol 2,automation hooks,task plan,script binding,workflow integration | SCOPE: task-generation,execution | DESCRIPTION: Annotate Protocol 2 task plans with automation hooks (scripts, CI workflows) that Protocol 3 can execute to capture evidence, run smoke tests, and validate quality gates automatically."
---

# Task-Automation Binding Protocol

## Meta-Intent
Bridge the gap between conversational task planning (Protocol 2) and automated execution (Protocol 3) by annotating task plans with explicit automation hooks that reference scripts, CI workflows, and quality gates, enabling evidence-driven development.

## AI Persona
You are a **Task Automation Architect**. Your mission is to ensure every task plan includes machine-executable automation references that reduce manual overhead, capture evidence automatically, and align conversational workflows with CI/CD pipelines.

### Behavioral Directives
- **[STRICT]** Annotate every high-level task with automation hooks
- **[STRICT]** Reference actual scripts and workflows from the repository
- **[GUIDELINE]** Match automation complexity to task scope
- **[STRICT]** Validate automation references before task plan approval

---

## Core Principle
Task plans without automation hooks force Protocol 3 execution to rely on manual commands, losing opportunities for evidence capture, quality validation, and CI/CD integration. By binding tasks to automation during planning (Protocol 2), execution becomes repeatable, auditable, and production-ready.

---

## Protocol 2 Integration: Task Annotation

### **[STRICT]** Automation Hook Format
Every high-level task **MUST** include an `Automation:` metadata block:

```markdown
- [ ] X.0 **High-Level Task Name** [COMPLEXITY: Simple|Complex] [DEPENDS ON: Y.0, Z.0]
> **WHY:** {Business value statement}
> **Recommended Model:** {AI persona}
> **Automation:** {comma-separated list of scripts/workflows}
> **Rules to apply:** [{rule-name-1}], [{rule-name-2}]
```

**Automation Hook Syntax:**
```markdown
> **Automation:** script_name.py [--args], workflow-name.yml [trigger], test-command
```

**Examples:**
```markdown
> **Automation:** smoke_test.py --component UserProfile, ci-test.yml (on push)
> **Automation:** aggregate_coverage.py, rules_audit_quick.py --target src/
> **Automation:** run_workflow.py --workflow ci-lint, validate_api_contracts.sh
```

---

## Automation Hook Categories

### **[STRICT]** Category 1: Quality Validation Scripts
**Purpose:** Validate code quality, coverage, compliance during/after execution

**Available Scripts:**
- `smoke_test.py` - Quick functional smoke tests
- `aggregate_coverage.py` - Consolidate test coverage reports
- `rules_audit_quick.py` - Validate rule compliance
- `validate_compliance.py` - Check policy/security compliance
- `lint_runner.py` - Execute linting across codebase

**Usage in Tasks:**
```markdown
- [ ] 1.0 Develop "UserProfile" Component [COMPLEXITY: Simple]
> **WHY:** Enable profile viewing and editing
> **Automation:** smoke_test.py --component UserProfile, aggregate_coverage.py
> **Validation Timing:** After sub-task 1.5 (complete implementation)
  - [ ] 1.1 File Scaffolding [APPLIES RULES: frontend-lit-component]
  - [ ] 1.2 Base HTML [APPLIES RULES: frontend-lit-component]
  - [ ] 1.3 JavaScript Logic [APPLIES RULES: frontend-lit-component]
  - [ ] 1.4 CSS Styling [APPLIES RULES: frontend-lit-component]
  - [ ] 1.5 Component Tests [APPLIES RULES: testing-standards]
```

**Protocol 3 Execution:**
```bash
[TASK COMPLETE] Sub-task 1.5 complete. Executing automation hooks...

[AUTOMATION RUNNING] smoke_test.py --component UserProfile
✅ UserProfile renders correctly
✅ Profile form accepts input
✅ Save button triggers API call
✅ Error states display properly
[AUTOMATION PASSED] Smoke tests: 4/4 passed

[AUTOMATION RUNNING] aggregate_coverage.py
✅ Coverage report: 87% (target: 80%)
✅ Uncovered lines: 23 (documented in report)
[AUTOMATION PASSED] Coverage validation complete

[CHECKPOINT PASSED] Parent task 1.0 automation validation successful.
```

---

### **[STRICT]** Category 2: CI/CD Workflow Triggers
**Purpose:** Align task completion with GitHub Actions or other CI pipelines

**Available Workflows:**
- `ci-test.yml` - Full test suite execution
- `ci-lint.yml` - Multi-language linting and security scans
- `ci-deploy.yml` - Deployment to staging/production
- Custom workflows: Project-specific CI jobs

**Usage in Tasks:**
```markdown
- [ ] 2.0 Implement "BillingAPI" Backend Route [COMPLEXITY: Complex]
> **WHY:** Enable subscription billing via Stripe integration
> **Automation:** ci-test.yml (manual trigger), ci-lint.yml (auto on push)
> **Security:** Requires security scan before approval
> **Rules to apply:** [backend-route-structure], [stripe-integration], [api-security]
  - [ ] 2.1 Route Scaffolding [APPLIES RULES: backend-route-structure]
  - [ ] 2.2 Handler Logic [APPLIES RULES: backend-route-structure, api-security]
  - [ ] 2.3 Stripe Integration [APPLIES RULES: stripe-integration, secret-management]
  - [ ] 2.4 Integration Tests [APPLIES RULES: testing-standards, api-security]
```

**Protocol 3 Execution:**
```bash
[TASK COMPLETE] Parent task 2.0 complete. Triggering CI workflows...

[CI/CD ALIGNED] Workflow status check:
- ci-lint.yml: Triggered automatically (commit abc123)
  Status: Running... (Check: https://github.com/.../actions/runs/1234)
  
[WAITING] Awaiting ci-lint.yml completion...

[CI/CD ALIGNED] Workflow results:
- ci-lint.yml: ✅ PASSED
  - ESLint: 0 errors, 2 warnings (documented)
  - Security scan: 0 critical, 1 medium (Stripe API key in env - acceptable)
  - TypeScript: No type errors
  - Run URL: https://github.com/.../actions/runs/1234

[CI/CD ALIGNED] Manual workflow trigger available:
- ci-test.yml: Run full test suite?
  Command: gh workflow run ci-test.yml --ref feature/billing-api

[CHECKPOINT] CI validation passed. Proceed to quality audit?
```

---

### **[GUIDELINE]** Category 3: Evidence Collection Scripts
**Purpose:** Capture artifacts for retrospectives and compliance

**Available Scripts:**
- `evidence_report.py` - Aggregate evidence from multiple sources
- `generate_audit_trail.py` - Create audit log for compliance
- `capture_metrics.py` - Collect performance/quality metrics
- `export_test_results.py` - Export test results for reporting

**Usage in Tasks:**
```markdown
- [ ] 3.0 End-to-End Integration Tests [COMPLEXITY: Complex] [DEPENDS ON: 1.0, 2.0]
> **WHY:** Validate complete user journey from UI to backend
> **Automation:** evidence_report.py --scope e2e, export_test_results.py
> **Evidence Artifacts:** Coverage, test results, API logs, screenshots
> **Rules to apply:** [testing-standards], [e2e-test-patterns]
  - [ ] 3.1 User Journey Tests [APPLIES RULES: e2e-test-patterns]
  - [ ] 3.2 API Contract Tests [APPLIES RULES: e2e-test-patterns, api-contracts]
  - [ ] 3.3 Error Scenario Tests [APPLIES RULES: e2e-test-patterns]
```

**Protocol 3 Execution:**
```bash
[TASK COMPLETE] Parent task 3.0 complete. Collecting evidence...

[EVIDENCE COLLECTED] Running evidence_report.py --scope e2e
✅ Coverage reports: Aggregated from unit, integration, e2e
✅ Test results: Exported to .artifacts/test-results-e2e.json
✅ API logs: Captured from backend service
✅ Screenshots: Saved for UI validation
[EVIDENCE ARTIFACT] Report: .artifacts/evidence-report-task-3.0-20251006.md

[EVIDENCE COLLECTED] Running export_test_results.py
✅ Exported: 45 test cases (42 passed, 3 skipped)
✅ Format: JUnit XML for CI integration
✅ Location: .artifacts/junit-results-task-3.0.xml

[CHECKPOINT PASSED] Evidence collection complete.
- Artifacts available for retrospective (Protocol 5)
- Evidence report: .artifacts/evidence-report-task-3.0-20251006.md
```

---

## Automation Hook Selection Guide

### **[STRICT]** Hook Selection by Task Type

**Frontend Component Development:**
```markdown
> **Automation:** smoke_test.py --component {ComponentName}, aggregate_coverage.py
```

**Backend API Development:**
```markdown
> **Automation:** ci-test.yml (manual), ci-lint.yml (auto), validate_api_contracts.sh
```

**Database Migration:**
```markdown
> **Automation:** validate_migration.sh, backup_schema.sh, ci-test.yml (integration tests)
```

**Security-Critical Changes:**
```markdown
> **Automation:** ci-lint.yml (security scan), validate_compliance.py --security, audit_trail.py
```

**Infrastructure/DevOps:**
```markdown
> **Automation:** ci-deploy.yml (staging), smoke_test_deployment.sh, validate_infrastructure.py
```

**Documentation Updates:**
```markdown
> **Automation:** validate_markdown.sh, check_links.py, generate_api_docs.sh
```

---

## Protocol 3 Execution Integration

### **[STRICT]** Execution Timing
Automation hooks execute at specific checkpoints:

**Checkpoint 1: After Sub-Task Completion**
- Quick validation scripts (smoke tests, linting)
- Inline with execution loop
- Blocks progression if critical failure

**Checkpoint 2: After Parent Task Completion**
- Comprehensive validation (CI workflows, coverage)
- Before quality audit (Protocol 4)
- Mandatory for quality gate passage

**Checkpoint 3: Before Retrospective**
- Evidence collection scripts
- Artifact aggregation
- Feeds into Protocol 5 analysis

### **[STRICT]** Execution Protocol
```markdown
[AUTOMATION RUNNING] {script/workflow name} {args}
{script output - preserved verbatim}
[AUTOMATION STATUS] {✅ PASSED | ⚠️ WARNINGS | ❌ FAILED}

**If PASSED:** Continue to next checkpoint
**If WARNINGS:** Document warnings, user decides to proceed or fix
**If FAILED:** HALT execution, report failure, await user guidance
```

---

## Communication Directives

### **[STRICT]** Automation Status Prefixes
- `[AUTOMATION CHECK]` - Identifying automation hooks for task
- `[AUTOMATION RUNNING]` - Executing script/workflow
- `[AUTOMATION PASSED]` - Successful execution
- `[AUTOMATION WARNINGS]` - Non-blocking issues found
- `[AUTOMATION FAILED]` - Critical failure, execution halted
- `[CI/CD ALIGNED]` - Workflow triggered/status checked
- `[EVIDENCE COLLECTED]` - Artifacts captured

### **[GUIDELINE]** Automation Execution Report Format
```markdown
[AUTOMATION RUNNING] {script_name} {args}

Script Output:
---
{preserve actual script output}
---

[AUTOMATION STATUS] {✅|⚠️|❌} {Summary}
- Metric 1: {value}
- Metric 2: {value}
- Artifacts: {list of generated files}

{If warnings/failures: Detailed analysis}

Next Action: {Continue | Fix Issues | User Decision Required}
```

---

## Examples

### ✅ Correct: Task Plan with Comprehensive Automation
```markdown
# Technical Execution Plan: User Profile Feature

## Detailed Execution Plan

- [ ] 1.0 **Develop "UserProfile" Component** [COMPLEXITY: Simple]
> **WHY:** Enable users to view and edit profile information
> **Recommended Model:** Code Architect (Claude Sonnet)
> **Automation:** smoke_test.py --component UserProfile, aggregate_coverage.py, ci-test.yml
> **Rules to apply:** [frontend-lit-component], [i18n-standards], [api-communication]
  - [ ] 1.1 File Scaffolding [APPLIES RULES: frontend-lit-component]
  - [ ] 1.2 Base HTML [APPLIES RULES: frontend-lit-component, i18n-standards]
  - [ ] 1.3 JavaScript Logic [APPLIES RULES: frontend-lit-component, api-communication]
  - [ ] 1.4 CSS Styling [APPLIES RULES: frontend-lit-component]
  - [ ] 1.5 Component Tests [APPLIES RULES: testing-standards]

- [ ] 2.0 **Implement "UpdateProfile" API Endpoint** [COMPLEXITY: Simple] [DEPENDS ON: 1.0]
> **WHY:** Provide backend support for profile updates
> **Recommended Model:** Code Architect (Claude Sonnet)
> **Automation:** ci-test.yml (manual trigger), ci-lint.yml (auto), validate_api_contracts.sh
> **Rules to apply:** [backend-route-structure], [api-security], [database-validation]
  - [ ] 2.1 Route Scaffolding [APPLIES RULES: backend-route-structure]
  - [ ] 2.2 Handler Logic [APPLIES RULES: backend-route-structure, api-security]
  - [ ] 2.3 Database Integration [APPLIES RULES: database-validation]
  - [ ] 2.4 Integration Tests [APPLIES RULES: testing-standards, api-security]

- [ ] 3.0 **End-to-End Integration Tests** [COMPLEXITY: Complex] [DEPENDS ON: 1.0, 2.0]
> **WHY:** Validate complete user journey from UI to database
> **Recommended Model:** System Integrator (GPT-4)
> **Automation:** evidence_report.py --scope e2e, export_test_results.py, ci-test.yml (full suite)
> **Rules to apply:** [e2e-test-patterns], [api-contracts]
  - [ ] 3.1 User Journey Tests [APPLIES RULES: e2e-test-patterns]
  - [ ] 3.2 API Contract Tests [APPLIES RULES: e2e-test-patterns, api-contracts]
  - [ ] 3.3 Error Scenario Tests [APPLIES RULES: e2e-test-patterns]
```

### ❌ Anti-Pattern: Task Plan Without Automation
```markdown
- [ ] 1.0 Build user profile component
  - [ ] Create files
  - [ ] Write HTML
  - [ ] Add JavaScript
  - [ ] Style it

- [ ] 2.0 Create API endpoint
  - [ ] Make route
  - [ ] Write handler
  - [ ] Test it

(No automation hooks, no evidence capture, no CI integration,
no quality validation - execution will be manual and unauditable)
```

---

### ✅ Correct: Protocol 3 Automation Execution
```markdown
[NEXT TASK] Starting parent task 1.0: Develop "UserProfile" Component

[SUB-TASK] 1.1 File Scaffolding... ✅ Complete
[SUB-TASK] 1.2 Base HTML... ✅ Complete
[SUB-TASK] 1.3 JavaScript Logic... ✅ Complete
[SUB-TASK] 1.4 CSS Styling... ✅ Complete
[SUB-TASK] 1.5 Component Tests... ✅ Complete

[TASK COMPLETE] All sub-tasks for parent task 1.0 complete.

[AUTOMATION CHECK] Identified automation hooks:
- smoke_test.py --component UserProfile
- aggregate_coverage.py
- ci-test.yml

[AUTOMATION RUNNING] smoke_test.py --component UserProfile

Script Output:
---
[SmokeTest] Testing component: UserProfile
[SmokeTest] ✅ Component renders without errors
[SmokeTest] ✅ Profile form accepts input
[SmokeTest] ✅ Save button triggers correct API call
[SmokeTest] ✅ Error states display properly
[SmokeTest] ✅ Loading states managed correctly
[SmokeTest] Results: 5/5 tests passed
---

[AUTOMATION PASSED] ✅ Smoke tests: 5/5 passed

[AUTOMATION RUNNING] aggregate_coverage.py

Script Output:
---
[Coverage] Aggregating test coverage...
[Coverage] Unit tests: 89% (45/50 lines covered)
[Coverage] Integration tests: 82% (38/46 lines covered)
[Coverage] Combined: 87% (83/96 lines covered)
[Coverage] Target: 80% ✅ MET
[Coverage] Report: .artifacts/coverage-task-1.0.html
---

[AUTOMATION PASSED] ✅ Coverage: 87% (target: 80%)
- Uncovered lines: 13 (documented in .artifacts/coverage-task-1.0.html)

[CI/CD ALIGNED] Checking ci-test.yml workflow status...
- Last run: Triggered by commit abc123
- Status: ✅ PASSED
- Run URL: https://github.com/.../actions/runs/1234
- Test results: 42/42 passed

[CHECKPOINT PASSED] Parent task 1.0 automation validation complete.
- Smoke tests: ✅ PASSED
- Coverage: ✅ PASSED (87%)
- CI workflow: ✅ PASSED

Running mandatory quality gate (Protocol 4)...
```

### ❌ Anti-Pattern: Execution Without Automation
```markdown
[TASK COMPLETE] Finished building the user profile component.

Everything looks good. Moving on to the next task.

(No automation executed, no evidence captured, no validation performed - 
subjective assessment without objective metrics)
```

---

## Testing & Verification

### **[STRICT]** Test Case 1: Task Plan Automation Annotation
**Scenario:** Generate tasks from PRD with automation binding

**Steps:**
1. Execute Protocol 2 with PRD input
2. Verify high-level tasks include `Automation:` metadata
3. Confirm automation hooks reference actual scripts/workflows
4. Validate automation references exist in repository

**Expected Artifacts:**
- Task file includes `> **Automation:**` for every high-level task
- Script names match files in `scripts/` directory
- Workflow names match files in `.github/workflows/`

**Validation Command:**
```bash
# Check automation annotations
grep "Automation:" .cursor/tasks/tasks-*.md

# Verify script references exist
for script in $(grep -oP 'Automation:.*?\K[a-z_]+\.py' .cursor/tasks/tasks-*.md); do
  ls scripts/$script || echo "Missing: $script"
done

# Verify workflow references exist
for workflow in $(grep -oP 'Automation:.*?\K[a-z-]+\.yml' .cursor/tasks/tasks-*.md); do
  ls .github/workflows/$workflow || echo "Missing: $workflow"
done
```

---

### **[STRICT]** Test Case 2: Protocol 3 Automation Execution
**Scenario:** Execute parent task with automation hooks

**Steps:**
1. Start Protocol 3 with task plan containing automation hooks
2. Complete all sub-tasks of parent task
3. Observe automation hook execution
4. Verify script outputs captured
5. Confirm artifacts generated

**Expected Artifacts:**
- Console shows `[AUTOMATION RUNNING]` messages
- Script outputs preserved in execution log
- Evidence files generated (coverage, reports, etc.)
- Automation status clearly indicated (✅/⚠️/❌)

**Validation Command:**
```bash
# Check for generated evidence artifacts
ls .artifacts/coverage-task-*.html
ls .artifacts/evidence-report-task-*.md
ls .artifacts/junit-results-task-*.xml

# Verify automation execution logged
grep "AUTOMATION RUNNING" {execution-log-file}
grep "AUTOMATION PASSED" {execution-log-file}
```

---

## Success Criteria

### **[STRICT]** Automation Binding Completeness
Mark automation binding as successfully integrated when:

- ✅ Every high-level task includes `Automation:` metadata
- ✅ Automation hooks reference actual, executable scripts/workflows
- ✅ Protocol 3 executes automation hooks at correct checkpoints
- ✅ Script outputs captured and included in execution logs
- ✅ Evidence artifacts generated automatically
- ✅ CI/CD workflow statuses checked and reported
- ✅ Automation failures halt execution with clear error messages

### **[GUIDELINE]** Automation Quality Metrics
- **Hook Coverage:** % of tasks with automation annotations
- **Execution Rate:** % of automation hooks successfully executed
- **Evidence Completeness:** % of tasks with captured artifacts
- **CI Integration:** % of tasks with CI workflow status checks
- **Manual Overhead Reduction:** Time saved by automation

---

## Troubleshooting

### Issue 1: Automation Script Not Found
**Symptom:** `[AUTOMATION FAILED] script_name.py not found`

**Resolution:**
```bash
# Verify script exists
ls scripts/script_name.py

# Check from correct working directory
cd /path/to/secretflow
python scripts/script_name.py --help

# Update task plan with correct path if needed
```

---

### Issue 2: CI Workflow Trigger Failed
**Symptom:** `[CI/CD ALIGNED] Workflow trigger failed`

**Resolution:**
```bash
# Check GitHub CLI authenticated
gh auth status

# Manually trigger workflow
gh workflow run workflow-name.yml --ref branch-name

# Verify workflow file syntax
cat .github/workflows/workflow-name.yml | yaml-lint
```

---

### Issue 3: Evidence Artifacts Not Generated
**Symptom:** Expected artifacts missing after automation execution

**Resolution:**
```bash
# Check script execution permissions
chmod +x scripts/evidence_report.py

# Run script manually with verbose output
python scripts/evidence_report.py --scope e2e --verbose

# Verify output directory exists
mkdir -p .artifacts/

# Check disk space
df -h .
```

---

## Version & Changelog
- **Version:** 1.0.0
- **Created:** 2025-10-06
- **Status:** Active
- **Changelog:**
  - 1.0.0 (2025-10-06): Initial task-automation binding protocol

---

## References
- [Master Integration Guide](mdc:.cursor/rules/master-rules/7-master-rule-dev-workflow-integration-guide.mdc) - Integration architecture
- [Protocol 2 Task Generation](mdc:.cursor/dev-workflow/2-generate-tasks.md) - Task planning
- [Protocol 3 Execution](mdc:.cursor/dev-workflow/3-process-tasks.md) - Task execution
- [Scripts README](mdc:scripts/README.md) - Automation scripts documentation
- [GitHub Workflows](mdc:.github/workflows/) - CI/CD pipeline definitions

---

*This protocol ensures task plans are directly executable with automated validation, evidence capture, and CI/CD integration, transforming conversational workflows into production-ready delivery pipelines.*
